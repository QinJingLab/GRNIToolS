% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ADMM.R
\name{ADMM}
\alias{ADMM}
\title{ADMM - alternating direction method of multiplier}
\usage{
ADMM(A, b, x0, sparsity, MaxIter = 200, epsilon = 1e-09)
}
\arguments{
\item{A}{Gene expression data of transcriptome factors (i.e. basis function in machine learning).}

\item{b}{Gene expression data of target genes (i.e. observation in machine learning).}

\item{x0}{Gene expression data of Chromatin immunoprecipitation or zero vector (i.e. initial iterative point in machine learning).}

\item{sparsity}{Sparsity level of solution.}

\item{MaxIter}{Maximum iteration used in calculation; we set default as 200.}

\item{epsilon}{Stopping rule in algorithm where the rule is \eqn{||Ax-b|{|_2^2} < \epsilon}; we set default as 1e-9.}
}
\description{
This function is the process of ADMM (alternating direction method of multipliers) aims to solve \eqn{l_1} regularization optimization model of linear regression (also called as Least Absolute Shrinkage and Selection Operator, LASSO).
}
\details{
Alternating direction method of multipliers (ADMM) was introduced by Yang and Zhang (2011) to solve the \eqn{l_1} regularization problem:
\deqn{{\min}_{x \in R^n} ||Ax - b|{|_2^2} + \lambda ||x|{|_1}}
The idea of ADMM is to apply the Gauss-Seidel decomposition technique to solve the joint minimization problem of the augmented Lagrangian function. By applying the Gauss-Seidel decomposition technique, the augmented Lagrangian function minimization can be calculated via analytical formulae consisting of a projected gradient descent step, a soft thresholding operator and an updating rule of multipliers.
}
\references{
Yang, J. and Zhang, Y. (2011). "Alternating direction algorithms for \eqn{l_1}-problems in compressive sensing", SIAM Journal on Scientific Computing, 33(1), 250-278.
}
\author{
Yaohua Hu <mayhhu@szu.edu.cn> 

Xinlin Hu <ttxinlinhu@qq.com>
}
